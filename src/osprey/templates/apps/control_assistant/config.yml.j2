# ============================================================
# {{ project_name }} Configuration
# ============================================================
# Accelerator Control System Assistant
# Based on ALS Assistant deployment (arXiv:2509.17255)
# ============================================================

# Project identity
project_name: "{{ project_name }}"

build_dir: ./build

# Root directory (absolute path)
project_root: {{ project_root }}

# Application Registry
registry_path: ./src/{{ package_name }}/registry.py

# ============================================================
# MODEL CONFIGURATION
# ============================================================
# The framework uses 8 different AI models for different roles.
# Configure each one explicitly for optimal performance and cost.

models:
  orchestrator:
    provider: {{ default_provider }}
    model_id: {{ default_model }}
    max_tokens: 4096
  response:
    provider: {{ default_provider }}
    model_id: {{ default_model }}
  classifier:
    provider: {{ default_provider }}
    model_id: {{ default_model }}
  approval:
    provider: {{ default_provider }}
    model_id: {{ default_model }}
  task_extraction:
    provider: {{ default_provider }}
    model_id: {{ default_model }}
  memory:
    provider: {{ default_provider }}
    model_id: {{ default_model }}
  python_code_generator:
    provider: {{ default_provider }}
    model_id: {{ default_model }}
    max_tokens: 8192  # Allow longer code generation
  time_parsing:
    provider: {{ default_provider }}
    model_id: {{ default_model }}
  channel_write:
    provider: {{ default_provider }}
    model_id: {{ default_model }}
    max_tokens: 2048  # For write operation parsing
  channel_finder:
    provider: {{ default_provider }}
    model_id: {{ default_model }}
    max_tokens: 4096  # For channel finder semantic search

# ============================================================
# API CONFIGURATION
# ============================================================
# API keys and endpoints for LLM providers

api:
  providers:
    cborg:
      api_key: ${CBORG_API_KEY}
      base_url: https://api.cborg.lbl.gov/v1
    amsc:
      api_key: ${AMSC_I2_API_KEY}
      base_url: https://api.i2-core.american-science-cloud.org/v1
    stanford:
      api_key: ${STANFORD_API_KEY}
      base_url: https://aiapi-prod.stanford.edu/v1
    openai:
      api_key: ${OPENAI_API_KEY}
      base_url: https://api.openai.com/v1
    anthropic:
      api_key: ${ANTHROPIC_API_KEY}
      base_url: https://api.anthropic.com
    google:
      api_key: ${GOOGLE_API_KEY}
      base_url: https://generativelanguage.googleapis.com/v1beta
    ollama:
      api_key: ollama
      base_url: ${OLLAMA_HOST:-http://localhost:11434}
      host: localhost
      port: 11434
    argo:
      api_key: ${ARGO_API_KEY}
      base_url: https://argo-bridge.cels.anl.gov

# ============================================================
# ENVIRONMENT VARIABLES
# ============================================================
# Set these in your .env file:
#
#   CBORG_API_KEY=your-cborg-key
#   AMSC_I2_API_KEY=your-amsc-key
#   STANFORD_API_KEY=...
#   ARGO_API_KEY=your-argo-key
#   OPENAI_API_KEY=sk-...
#   ANTHROPIC_API_KEY=sk-ant-...
#   GOOGLE_API_KEY=...
#   TZ=America/Los_Angeles
#
# Optional (for advanced use cases):
#   PROJECT_ROOT=/path/to/override  # Override project_root from config
#   LOCAL_PYTHON_VENV=/path/to/venv/bin/python  # Override Python environment

# ============================================================
# CONTAINER RUNTIME
# ============================================================
# Container runtime selection for deployment
# Options: auto (default), docker, podman
# - auto: Tries Docker first, then Podman
# - docker: Use Docker Desktop (recommended for macOS/Windows)
# - podman: Use Podman (recommended for Linux/security-focused)
# Can also override via CONTAINER_RUNTIME environment variable

container_runtime: auto

# ============================================================
# DEPLOYED SERVICES
# ============================================================
# Services to deploy when running framework deploy up

deployed_services:
  - jupyter
  - open_webui
  - pipelines
  - postgresql
  - ariel_web

# ============================================================
# DEPLOYMENT CONFIGURATION
# ============================================================
# Security settings for container deployment

deployment:
  # Network bind address for exposed service ports
  # Default: 127.0.0.1 (localhost only - SECURE, recommended for development)
  # Use 0.0.0.0 to expose to all network interfaces (for cloud/production)
  # WARNING: Setting to 0.0.0.0 exposes services to the network!
  # Only use 0.0.0.0 when proper authentication is configured.
  bind_address: "127.0.0.1"

# ============================================================
# SERVICE CONFIGURATION
# ============================================================
# Framework services for execution and user interface

services:
  # Jupyter - Python execution environment with EPICS support
  jupyter:
    path: ./services/jupyter
    containers:
      read:
        name: jupyter-read
        hostname: jupyter-read
        port_host: 8088
        port_container: 8088
        execution_modes: ["read_only"]
      write:
        name: jupyter-write
        hostname: jupyter-write
        port_host: 8089
        port_container: 8088
        execution_modes: ["write_access"]
    copy_src: true  # Copies user application from src/
    render_kernel_templates: true
    # additional_dirs: []  # Add custom directories to copy if needed

  # Open WebUI - Primary user interface frontend
  open_webui:
    path: ./services/open-webui
    hostname: localhost
    port_host: 8080
    port_container: 8080

  # Pipelines - OpenWebUI pipeline interface server
  pipelines:
    path: ./services/pipelines
    port_host: 9099
    port_container: 9099
    copy_src: true  # Copies user application from src/
    # additional_dirs: []  # Add custom directories to copy if needed

  # PostgreSQL for ARIEL logbook search
  postgresql:
    path: ./services/postgresql
    database_name: ariel
    username: ariel
    password: ariel
    port_host: 5432

  # ARIEL Web Interface
  ariel_web:
    path: ./services/ariel-web
    port_host: 8085
    port_container: 8085

# ============================================================
# SAFETY CONTROLS
# ============================================================

# Approval workflow for sensitive operations
approval:
  global_mode: "selective"   # Options: disabled | selective | all_capabilities
  capabilities:
    python_execution:
      enabled: true
      mode: "control_writes"   # Options: disabled | control_writes | all_code
    memory:
      enabled: true

# Execution limits and safety controls
execution_control:
  # Agent control - Runtime behavior configuration
  agent_control:
    orchestration_mode: plan_first          # Options: plan_first | react
    task_extraction_bypass_enabled: false    # Skip LLM task extraction
    capability_selection_bypass_enabled: false  # Skip LLM capability selection

  # Execution safety limits and control flow
  limits:
    max_reclassifications: 1          # Maximum times a task can be reclassified
    max_planning_attempts: 2          # Maximum number of planning attempts
    max_step_retries: 3               # Maximum retries per step before failing
    max_execution_time_seconds: 3000  # Maximum execution time (50 minutes)
    graph_recursion_limit: 100        # LangGraph recursion limit
    max_concurrent_classifications: 5 # Maximum concurrent LLM classification requests

# ============================================================
# CONTROL SYSTEM & ARCHIVER CONFIGURATION
# Issue #18 - Control System Abstraction
# ============================================================
# Unified configuration for control systems and archivers.
# Supports mock mode (development) and production mode (EPICS, etc.)
#
# Development Mode: Works instantly with any channel names (no hardware required)
# Production Mode: Connects to real control system and archiver
# Switch modes by changing 'type' field

control_system:
  # Control system type - determines which runtime connector to use
  # Note: Pattern detection is control-system-agnostic (same for all types)
  # Options:
  #   - mock: Development mode (accepts any channel names, no hardware)
  #   - epics: Production EPICS control system
  type: mock  # <-- Change to 'epics' for production

  # Write Access Control - Master safety switch for hardware control
  # This is OPERATOR-LEVEL authorization (always hard-stops if false)
  writes_enabled: true  # Tutorial default: enabled for mock connector
                        # For production: carefully review hardware implications before enabling
                        # If disabled, ALL writes are blocked (non-configurable hard error)

  # Runtime Channel Limits Checking
  limits_checking:
    enabled: true
    database_path: "src/{{ package_name }}/data/channel_limits.json"
    allow_unlisted_channels: true  # Permissive mode for tutorial
    on_violation: "skip"   # Options: "error" | "skip" (DEFAULT)
                           # - error: Strict - ANY violation blocks entire operation
                           # - skip: Resilient - skip invalid writes, continue with valid ones
                           # Production: "skip" recommended for multi-channel operations

  # Write Verification Configuration
  # Configures how channel writes are verified for accuracy
  write_verification:
    enabled: true  # Enable verification by default
    default_level: callback  # Options: none, callback, readback
                            # - none: Fast write, no verification
                            # - callback: Control system confirms processing (e.g., EPICS IOC callback)
                            # - readback: Full verification with readback comparison
    default_tolerance_percent: 0.1  # Default tolerance: 0.1% (one per mil)
                                    # Only used for "readback" verification level
                                    # Can be overridden per-channel in channel_limits.json
    timeout: 5.0  # Timeout for verification operations
    fail_on_mismatch: false  # false: log warning, true: raise error on verification failure

  # Pattern Detection (Layer 1 - Security)
  # Purpose: Detect ALL control system operations in generated code for approval workflows
  #
  # SECURITY: Catches both approved API usage AND circumvention attempts
  # The framework automatically detects:
  #   - Approved: osprey.runtime API (write_channel, read_channel) - has all safety features
  #   - Security: Direct library calls (epics.caput, tango.DeviceProxy, etc.) - bypasses safety
  #   - Future: Other control systems (LabVIEW, custom) - comprehensive detection
  #
  # This ensures LLM-generated code cannot bypass connector safety features (limits, verification)
  # by directly importing control system libraries.
  #
  # Advanced: Extend patterns for custom control system libraries (rarely needed)
  # Uncomment and add to customize:
  # patterns:
  #   write:
  #     - 'my_custom_cs_lib\.write\('
  #   read:
  #     - 'my_custom_cs_lib\.read\('

  # Runtime Connectors (Layer 2)
  # Used by: Capabilities (channel_read, channel_write, archiver_retrieval)
  #          and osprey.runtime API (read_channel, write_channel)
  # Purpose: Actual I/O operations to control systems with built-in safety features
  connector:
    timeout: 5.0  # Default timeout in seconds

    # Mock connector uses sensible defaults (no configuration needed)
    # Advanced users can configure: response_delay_ms, noise_level, enable_writes

    # EPICS connector (production mode)
    epics:
      timeout: 5.0
      gateways:
        read_only:
          address: cagw-alsdmz.als.lbl.gov
          port: 5064
          # use_name_server: Set to true for SSH tunnels (uses EPICS_CA_NAME_SERVERS)
          #                 Set to false for direct gateway (uses EPICS_CA_ADDR_LIST)
          #                 Defaults to false. Can be overridden via environment variable.
          use_name_server: false
        write_access:
          address: cagw-alsdmz.als.lbl.gov
          port: 5084
          use_name_server: false

# Archiver Configuration
archiver:
  # Archiver type determines which connector to use
  # Options:
  #   - mock_archiver: Development mode (generates synthetic data)
  #   - epics_archiver: EPICS Archiver Appliance
  type: mock_archiver  # <-- Change to 'epics_archiver' for production

  # Mock archiver uses sensible defaults (no configuration needed)
  # Advanced users can configure: sample_rate_hz, noise_level

  # EPICS Archiver Appliance (production mode)
  epics_archiver:
    url: https://archiver.als.lbl.gov:8443
    timeout: 60              # Timeout for archiver requests in seconds

# Migration from tutorial to production:
# 1. Change control_system.type from 'mock' to 'epics'
# 2. Change archiver.type from 'mock_archiver' to 'epics_archiver'
# 3. Test with: osprey run
# Your code doesn't need to change!

# ============================================================
# MACHINE STATE CONFIGURATION
# ============================================================
machine_state:
  channels_file: "data/machine_state_channels.json"

# ============================================================
# CHANNEL FINDER CONFIGURATION
# ============================================================
# Natural language channel finding service configuration
# Pipeline mode: {{ channel_finder_mode }}

channel_finder:
  # Active pipeline mode
  pipeline_mode: {{ default_pipeline }}  # "in_context", "hierarchical", or "middle_layer"

  # Explicit channel address validation mode
  # Controls how explicit PV addresses in queries are validated against the channel finder database
  # Options:
  #   - lenient (DEFAULT): Accept all explicit addresses, warn if not in database
  #                       Best for production - trusts user-provided addresses while providing feedback
  #                       Use case: Database has 200 curated PVs, but users may reference any of 5000+ actual PVs
  #   - strict: Only accept channels that exist in database
  #            Best for testing - ensures all channels are documented
  #   - skip: No validation, accept all addresses immediately (fastest)
  #          Best for performance - when you fully trust user inputs
  explicit_validation_mode: lenient

  # Pipeline-specific configurations
  pipelines:
{% if enable_in_context %}
    # In-Context Pipeline (Semantic Search)
    # Best for: few hundred channels (example from UCSB FEL)
    in_context:
      database:
        type: template                # Database type: "template" (with device family expansion) or "flat" (simple list)
        path: src/{{ package_name }}/data/channel_databases/in_context.json
        presentation_mode: template    # How to present device families to LLM:
                                      # "explicit" - list all expanded channel names (BPM01XPosition, BPM02XPosition, ...)
                                      # "template" - use pattern notation (BPM{01-10}{XPosition,YPosition})

      processing:
        chunk_dictionary: false       # Whether to chunk the database for large channel lists
                                      # false - send entire database in one LLM call (faster, recommended for <200 channels)
                                      # true - split database into chunks and search each independently
        chunk_size: 50                # Number of channels per chunk when chunk_dictionary=true
        max_correction_iterations: 2  # Maximum LLM correction attempts when invalid channels are found (0-3 recommended)

      # Benchmark dataset for this pipeline
      benchmark:
        dataset_path: src/{{ package_name }}/data/benchmarks/datasets/in_context_benchmark.json
        # In-context pipeline benchmark using UCSB FEL channels (30 queries)
{% endif %}
{% if enable_hierarchical %}
    # Hierarchical Pipeline (Structured Navigation)
    # Best for: >1,000 channels with clean naming convention
    hierarchical:
      database:
        type: hierarchical            # Always "hierarchical" for this pipeline mode
        path: src/{{ package_name }}/data/channel_databases/hierarchical.json
                                      # Database must follow nested structure: system → family → device → field → subfield

      # Benchmark dataset for this pipeline
      benchmark:
        dataset_path: src/{{ package_name }}/data/benchmarks/datasets/hierarchical_benchmark.json
        # Hierarchical pipeline benchmark with multi-level channel structure (47 queries)
{% endif %}
{% if enable_middle_layer %}
    # Middle Layer Pipeline (React Agent with MML Database)
    # Best for: MATLAB Middle Layer (MML) style databases, functional hierarchy (System→Family→Field), device/sector filtering
    middle_layer:
      database:
        type: middle_layer            # Always "middle_layer" for this pipeline mode
        path: src/{{ package_name }}/data/channel_databases/middle_layer.json
                                      # Database must follow MML structure: System → Family → Field → ChannelNames
                                      # Supports optional _description fields and device/sector filtering via DeviceList

      # Benchmark dataset for this pipeline
      benchmark:
        dataset_path: src/{{ package_name }}/data/benchmarks/datasets/middle_layer_benchmark.json
        # Middle layer pipeline benchmark with functional hierarchy (35 queries, 174 channels)
{% endif %}

  # Benchmarking configuration
  # The active dataset is automatically determined by the selected pipeline_mode
  # Each pipeline defines its own benchmark dataset in channel_finder.pipelines.<mode>.benchmark
  # You can override the dataset via CLI: python scripts/run_benchmarks.py --dataset custom_dataset
  benchmark:
    # Execution settings
    execution:
      runs_per_query: 1                 # Number of times to run each query
      delay_between_runs: 0             # Delay in seconds between runs
      continue_on_error: true           # Continue even if some queries fail
      max_concurrent_queries: 5         # Maximum parallel queries
      query_selection: all              # "all" or [0,1,2] or range notation

    # Output settings
    output:
      results_dir: src/{{ package_name }}/data/benchmarks/results
      save_incremental: true            # Save results after each query
      include_detailed_metrics: true    # Include detailed timing/cost metrics

    # Evaluation thresholds
    evaluation:
      exact_match_weight: 1.0
      partial_match_weight: 0.5

# ============================================================
# EXECUTION INFRASTRUCTURE
# ============================================================

# Python execution configuration
execution:
  execution_method: "local"              # Options: local | container
  python_env_path: {{ current_python_env }}  # Path to Python with dependencies

  # Code generation configuration
  code_generator: "{{ code_generator | default('basic') }}"  # Options: basic | claude_code
  generators:
    basic:
      # Reference the model config defined in 'models' section
      model_config_name: "python_code_generator"
      save_prompts: true  # Save prompts and responses for transparency/debugging
      # Or define inline configuration here (overrides model_config_name):
      # provider: cborg
      # model_id: anthropic/claude-haiku
      # temperature: 0.7

    # Claude Code configuration (requires claude-agent-sdk)
    claude_code:
      claude_config_path: "claude_generator_config.yml"  # Path to detailed config
      profile: "fast"        # Options: fast (DEFAULT, single-phase) | robust (multi-phase)
      # Advanced settings are in claude_generator_config.yml

  # EPICS infrastructure (DEPRECATED - use control_system section above)
  # Maintained for backward compatibility during migration
  epics:
    timeout: 5.0
    gateways:
      read_only:
        address: cagw-alsdmz.als.lbl.gov
        port: 5064
      write_access:
        address: cagw-alsdmz.als.lbl.gov
        port: 5084

  # Python/Jupyter execution modes
  modes:
    read_only:
      kernel_name: "python3-epics-readonly"
      gateway: "read_only"
      allows_writes: false
      environment:
        EPICS_CA_AUTO_ADDR_LIST: "NO"
        EPICS_EXECUTION_MODE: "read_only"
    write_access:
      kernel_name: "python3-epics-write"
      gateway: "write_access"
      allows_writes: true
      requires_approval: true
      environment:
        EPICS_CA_AUTO_ADDR_LIST: "NO"
        EPICS_EXECUTION_MODE: "write_access"

# Python executor settings
python_executor:
  max_generation_retries: 3
  max_execution_retries: 3
  execution_timeout_seconds: 600

# ============================================================
# APPLICATION METADATA
# ============================================================

pipeline:
  name: "{{ project_name }}"
  startup_hooks: []

# ============================================================
# CLI CONFIGURATION
# ============================================================
# Customize the command-line interface appearance

cli:
  # Theme selection - Options: default | vulcan | custom
  theme: "default"

  # Custom theme colors (only used when theme: custom)
  # Uncomment and modify to create your own color scheme
  # custom_theme:
  #   primary: "#C75F71"     # Main brand color
  #   success: "#9988A1"     # Success messages
  #   accent: "#F0B8B8"      # Interactive elements
  #   command: "#9988A1"     # Shell commands
  #   path: "#A2AE9D"        # File paths
  #   info: "#9988A1"        # Info messages

  # Custom ASCII banner (optional)
  # Uncomment to override the default Osprey banner
  # banner: |
  #   Your custom ASCII art here


# Note: Theme system logging uses the existing 'base' logger color
# No additional logging configuration needed

# ============================================================
# DEVELOPMENT & DEBUGGING
# ============================================================

development:
  # If true, exceptions raised directly instead of wrapped
  raise_raw_errors: false

  # Prompt debugging configuration
  prompts:
    show_all: false      # Print prompts to console
    print_all: true      # Save prompts to files
    latest_only: true    # Keep only latest prompt version

  # LLM API call logging configuration
  # Captures raw inputs/outputs to get_chat_completion with caller metadata
  api_calls:
    save_all: true             # Save all API inputs/outputs to files
    latest_only: true          # Keep only latest call per function (false = timestamped files)
    include_stack_trace: false # Include full Python stack trace in metadata

# ============================================================
# LOGGING
# ============================================================

logging:
  # Rich logging traceback settings
  rich_tracebacks: false
  show_traceback_locals: false
  show_full_paths: false

  # Component colors (for terminal output)
  logging_colors:
    # Core framework components
    base: 'white'
    context: "light_steel_blue1"
    router: "bright_magenta"
    orchestrator: "cyan"
    monitor: "dark_orange3"
    classifier: "light_salmon1"
    task_extraction: "thistle1"
    error: "red"
    gateway: "light_salmon1"
    approval: "light_salmon1"

    # Framework capabilities
    time_range_parsing: "dodger_blue1"
    memory: "light_salmon1"
    python: "light_salmon1"

    # Communication capabilities
    respond: "thistle1"
    clarify: "thistle1"
    message_generator: "thistle1"

    # Execution infrastructure
    python_generator: "sandy_brown"
    python_analyzer: "salmon1"
    python_executor: "misty_rose1"
    python_workflow: "light_steel_blue1"
    python_services: "light_steel_blue1"

    # Interface components
    cli: "deep_sky_blue1"
    pipeline: "deep_sky_blue1"

    # Accelerator-specific components
    channel_finding: "light_green"
    channel_read: "light_cyan1"
    channel_write: "light_salmon3"
    archiver_retrieval: "turquoise2"

    # ARIEL logbook search
    ariel: "medium_purple1"

# ============================================================
# SYSTEM CONFIGURATION
# ============================================================

system:
  # Timezone configuration for all containers and services
  timezone: ${TZ:-America/Los_Angeles}

# File paths - Data storage organization
file_paths:
  agent_data_dir: _agent_data
  executed_python_scripts_dir: executed_scripts
  execution_plans_dir: execution_plans
  user_memory_dir: user_memory
  registry_exports_dir: registry_exports
  prompts_dir: prompts
  api_calls_dir: api_calls
  checkpoints: checkpoints

# ============================================================
# ARIEL - Electronic Logbook Search
# ============================================================
# ARIEL provides semantic search over electronic logbooks.
# Configure the logbook source and search/embedding options below.
#
# Provider credentials are defined in api.providers section above.
# ARIEL references providers by name (e.g., "ollama", "cborg").

ariel:
  database:
    uri: postgresql://ariel:ariel@localhost:5432/ariel

  default_max_results: 10

  # Ingestion - demo logbook data (bundled with template)
  # For production: change adapter and source_url to your logbook system
  ingestion:
    adapter: generic_json
    source_url: src/{{ package_name }}/data/logbook_seed/demo_logbook.json

  # Search modules (leaf-level search functions)
  # provider: references api.providers for credentials (api_key, base_url)
  # Both keyword and semantic are enabled by default.
  # If Ollama/pgvector are unavailable, ARIEL degrades gracefully to keyword-only.
  search_modules:
    keyword:
      enabled: true
    semantic:
      enabled: true      # Degrades gracefully if Ollama/pgvector unavailable
      provider: ollama    # References api.providers.ollama
      model: nomic-embed-text

  # Pipelines (compose search modules into higher-level strategies)
  # retrieval_modules: which search modules each pipeline uses
  pipelines:
    rag:
      enabled: true
      retrieval_modules: [keyword, semantic]  # Semantic auto-disabled if unavailable
    agent:
      enabled: true
      retrieval_modules: [keyword, semantic]  # Semantic auto-disabled if unavailable

  # Enhancement modules
  # provider: references api.providers for credentials
  enhancement_modules:
    # Semantic processor: LLM-based keyword extraction and summarization
    # Enables better keyword search and RAG context
    semantic_processor:
      enabled: false  # Requires LLM API calls. Enable for better search quality.
      provider: {{ default_provider }}  # References api.providers
      model:
        provider: {{ default_provider }}
        model_id: {{ default_model }}
        max_tokens: 256

    # Text embedding: Vector embeddings for semantic search
    # Degrades gracefully if Ollama or pgvector is unavailable
    text_embedding:
      enabled: true   # Degrades gracefully if Ollama/pgvector unavailable
      provider: ollama  # References api.providers.ollama
      models:
        - name: nomic-embed-text
          dimension: 768

  # Default embedding provider (fallback for modules without explicit provider)
  embedding:
    provider: ollama

  # ReAct agent configuration
  # provider: references api.providers for credentials (api_key, base_url)
  reasoning:
    provider: cborg  # References api.providers.cborg for credentials
    model_id: anthropic/claude-haiku
    max_iterations: 5
    temperature: 0.1
    total_timeout_seconds: 120
