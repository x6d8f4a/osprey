# ============================================================
# {{ project_name }} Configuration
# ============================================================
# Complete, self-contained configuration.
# All framework settings included for transparency and control.
# ============================================================

# Project identity
project_name: "{{ project_name }}"

build_dir: ./build

# Root directory (absolute path)
project_root: {{ project_root }}

# Application Registry
registry_path: ./src/{{ package_name }}/registry.py

# ============================================================
# CONTAINER RUNTIME
# ============================================================
# Container runtime selection for deployment
# Options: auto (default), docker, podman
# - auto: Tries Docker first, then Podman
# - docker: Use Docker Desktop (recommended for macOS/Windows)
# - podman: Use Podman (recommended for Linux/security-focused)
# Can also override via CONTAINER_RUNTIME environment variable

container_runtime: auto

# ============================================================
# MODEL CONFIGURATION
# ============================================================
# The framework uses 8 different AI models for different roles.
# Configure each one explicitly. You can use the same model for
# multiple roles or specialize each one.
#
# Common providers: cborg, openai, anthropic, google, ollama
# Budget tip: Use anthropic/claude-haiku via cborg for most roles

models:
  orchestrator:
    provider: {{ default_provider | default("cborg") }}
    model_id: {{ default_model | default("anthropic/claude-haiku") }}
    max_tokens: 4096
  response:
    provider: {{ default_provider | default("cborg") }}
    model_id: {{ default_model | default("anthropic/claude-haiku") }}
  classifier:
    provider: {{ default_provider | default("cborg") }}
    model_id: {{ default_model | default("anthropic/claude-haiku") }}
  approval:
    provider: {{ default_provider | default("cborg") }}
    model_id: {{ default_model | default("anthropic/claude-haiku") }}
  task_extraction:
    provider: {{ default_provider | default("cborg") }}
    model_id: {{ default_model | default("anthropic/claude-haiku") }}
  memory:
    provider: {{ default_provider | default("cborg") }}
    model_id: {{ default_model | default("anthropic/claude-haiku") }}
  python_code_generator:
    provider: {{ default_provider | default("cborg") }}
    model_id: {{ default_model | default("anthropic/claude-haiku") }}
  time_parsing:
    provider: {{ default_provider | default("cborg") }}
    model_id: {{ default_model | default("anthropic/claude-haiku") }}

# ============================================================
# DEPLOYMENT CONFIGURATION
# ============================================================
# Security settings for container deployment

deployment:
  # Network bind address for exposed service ports
  # Default: 127.0.0.1 (localhost only - SECURE, recommended for development)
  # Use 0.0.0.0 to expose to all network interfaces (for cloud/production)
  # WARNING: Setting to 0.0.0.0 exposes services to the network!
  # Only use 0.0.0.0 when proper authentication is configured.
  bind_address: "127.0.0.1"

# ============================================================
# SERVICE CONFIGURATION
# ============================================================
# Framework services for execution and user interface

services:
  # Jupyter - Python execution environment
  jupyter:
    path: ./services/jupyter
    containers:
      read:
        name: jupyter-read
        hostname: jupyter-read
        port_host: 8088
        port_container: 8088
        execution_modes: ["read_only"]
      write:
        name: jupyter-write
        hostname: jupyter-write
        port_host: 8089
        port_container: 8088
        execution_modes: ["write_access"]
    copy_src: true  # Copies user application from src/
    render_kernel_templates: true
    # additional_dirs: []  # Add custom directories to copy if needed

  # Open WebUI - Primary user interface frontend
  open_webui:
    path: ./services/open-webui
    hostname: {{ hostname | default("localhost") }}
    port_host: 8080
    port_container: 8080

  # Pipelines - OpenWebUI pipeline interface server
  pipelines:
    path: ./services/pipelines
    port_host: 9099
    port_container: 9099
    copy_src: true  # Copies user application from src/
    # additional_dirs: []  # Add custom directories to copy if needed

# ============================================================
# DEPLOYED SERVICES
# ============================================================
# Services to deploy when running framework deploy up

deployed_services:
  - jupyter
  - open_webui
  - pipelines

# ============================================================
# SAFETY CONTROLS
# ============================================================

# Approval workflow for sensitive operations
approval:
  global_mode: "selective"   # Options: disabled | selective | all_capabilities
  capabilities:
    python_execution:
      enabled: true
      mode: "control_writes"   # Options: disabled | control_writes | all_code
    memory:
      enabled: true

# Execution limits and safety controls
execution_control:
  # Agent control - Runtime behavior configuration
  agent_control:
    orchestration_mode: plan_first          # Options: plan_first | react
    task_extraction_bypass_enabled: false    # Skip LLM task extraction
    capability_selection_bypass_enabled: false  # Skip LLM capability selection

  # Execution safety limits and control flow
  limits:
    max_reclassifications: 1          # Maximum times a task can be reclassified
    max_planning_attempts: 2          # Maximum number of planning attempts
    max_step_retries: 3               # Maximum retries per step before failing
    max_execution_time_seconds: 3000  # Maximum execution time (50 minutes)
    graph_recursion_limit: 100        # LangGraph recursion limit
    max_concurrent_classifications: 5 # Maximum concurrent LLM classification requests


# ============================================================
# SYSTEM CONFIGURATION
# ============================================================

system:
  # Timezone configuration for all containers and services
  timezone: ${TZ:-America/Los_Angeles}

# File paths - Data storage organization
file_paths:
  agent_data_dir: _agent_data
  executed_python_scripts_dir: executed_scripts
  execution_plans_dir: execution_plans
  user_memory_dir: user_memory
  registry_exports_dir: registry_exports
  prompts_dir: prompts
  api_calls_dir: api_calls
  checkpoints: checkpoints

# ============================================================
# EXECUTION INFRASTRUCTURE
# ============================================================

# Python execution configuration
execution:
  execution_method: "local"              # Options: local | container
  python_env_path: {{ current_python_env }}  # Path to Python with dependencies

  # Code generation configuration
  code_generator: "{{ code_generator | default('basic') }}"  # Options: basic | claude_code
  generators:
    basic:
      # Reference the model config defined in 'models' section
      model_config_name: "python_code_generator"
      # Or define inline configuration here (overrides model_config_name):
      # provider: cborg
      # model_id: anthropic/claude-haiku
      # temperature: 0.7

    # Claude Code configuration (requires claude-agent-sdk)
    claude_code:
      claude_config_path: "claude_generator_config.yml"  # Path to detailed config
      profile: "fast"    # Options: fast | robust (or custom profiles you define)
      # Advanced settings are in claude_generator_config.yml

  # --------------------------------------------------------------------------
  # DEPLOYMENT INFRASTRUCTURE (required for deployed Jupyter/container services)
  # These settings configure the container execution environment.
  # --------------------------------------------------------------------------

  # EPICS infrastructure (for hardware control applications)
  epics:
    timeout: 5.0
    gateways:
      read_only:
        address: your-gateway.example.com  # Replace with your EPICS gateway address
        port: 5064
      write_access:
        address: your-gateway.example.com  # Replace with your EPICS gateway address
        port: 5084

  # Python/Jupyter execution modes (configures Jupyter kernel environments)
  modes:
    read_only:
      kernel_name: "python3-epics-readonly"
      gateway: "read_only"
      allows_writes: false
      environment:
        EPICS_CA_AUTO_ADDR_LIST: "NO"
        EPICS_EXECUTION_MODE: "read_only"
    write_access:
      kernel_name: "python3-epics-write"
      gateway: "write_access"
      allows_writes: true
      requires_approval: true
      environment:
        EPICS_CA_AUTO_ADDR_LIST: "NO"
        EPICS_EXECUTION_MODE: "write_access"

# ============================================================
# CONTROL SYSTEM CONFIGURATION
# ============================================================
# Control system type, write access, limits checking, and verification
# Default: mock connector (no hardware required, safe for development)

control_system:
  # Control system type - determines which runtime connector to use
  # Note: Pattern detection is control-system-agnostic (same for all types)
  type: "mock"  # Options: mock | epics

  # Write Access Control - Master safety switch
  writes_enabled: false  # Set true only for production hardware control

  # Runtime Channel Limits Checking
  limits_checking:
    enabled: false  # Enable for production safety
    database_path: null  # Path to channel_limits.json
    allow_unlisted_channels: true
    on_violation: "skip"  # Options: error | skip

  # Write Verification Configuration
  write_verification:
    enabled: false  # Enable verification for production
    default_level: "callback"  # Options: none | callback | readback
    default_tolerance_percent: 0.1
    timeout: 5.0
    fail_on_mismatch: false

  # Connector-specific configuration
  connector:
    mock:
      simulate_delays: false
    epics:
      timeout: 5.0
      gateways:
        read_only:
          address: your-gateway.example.com  # Replace with your EPICS gateway address
          port: 5064
          use_name_server: false  # Set true for SSH tunnels
        write_access:
          address: your-gateway.example.com  # Replace with your EPICS gateway address
          port: 5084
          use_name_server: false

  # Pattern Detection (Security Layer)
  # Purpose: Detect ALL control system operations for approval workflows
  # Security: Catches approved API AND circumvention attempts (direct library calls)
  # Framework Default: Detects osprey.runtime API + EPICS/Tango/LabVIEW direct calls
  # Advanced: Extend for custom control system libraries (rarely needed)
  # patterns:
  #   write: ['my_custom_cs_lib\.write\(']
  #   read: ['my_custom_cs_lib\.read\(']

  # Runtime Connectors
  # Used by: Capabilities and osprey.runtime API
  # Purpose: Actual I/O operations with built-in safety features

# Python executor settings
python_executor:
  max_generation_retries: 3
  max_execution_retries: 3
  execution_timeout_seconds: 600


# ============================================================
# APPLICATION METADATA
# ============================================================

pipeline:
  name: "{{ project_name }}"
  startup_hooks: []

# ============================================================
# CLI CONFIGURATION
# ============================================================
# Customize the command-line interface appearance

cli:
  # Theme selection - Options: default | vulcan | custom
  theme: "default"

  # Custom theme colors (only used when theme: custom)
  # Uncomment and modify to create your own color scheme
  # custom_theme:
  #   primary: "#C75F71"     # Main brand color
  #   success: "#9988A1"     # Success messages
  #   accent: "#F0B8B8"      # Interactive elements
  #   command: "#9988A1"     # Shell commands
  #   path: "#A2AE9D"        # File paths
  #   info: "#9988A1"        # Info messages

  # Custom ASCII banner (optional)
  # Uncomment to override the default Osprey banner
  # banner: |
  #   Your custom ASCII art here


# Note: Theme system logging uses the existing 'base' logger color
# No additional logging configuration needed

# ============================================================
# DEVELOPMENT & DEBUGGING
# ============================================================

development:
  # If true, exceptions raised directly instead of wrapped
  raise_raw_errors: false

  # Prompt debugging configuration
  prompts:
    show_all: false      # Print prompts to console
    print_all: true      # Save prompts to files
    latest_only: true    # Keep only latest prompt version

  # LLM API call logging configuration
  # Captures raw inputs/outputs to get_chat_completion with caller metadata
  api_calls:
    save_all: true             # Save all API inputs/outputs to files
    latest_only: true          # Keep only latest call per function (false = timestamped files)
    include_stack_trace: false # Include full Python stack trace in metadata

# ============================================================
# LOGGING
# ============================================================

logging:
  # Rich logging traceback settings
  rich_tracebacks: false
  show_traceback_locals: false
  show_full_paths: false

  # Component colors (for terminal output)
  logging_colors:
    # Core framework components
    base: 'white'
    registry: "sky_blue2"
    context: "light_steel_blue1"
    router: "bright_magenta"
    orchestrator: "cyan"
    monitor: "dark_orange3"
    classifier: "light_salmon1"
    task_extraction: "thistle1"
    error: "red"
    gateway: "light_salmon1"
    approval: "light_salmon1"

    # Framework capabilities
    time_range_parsing: "dodger_blue1"
    memory: "light_salmon1"
    python: "light_salmon1"

    # Communication capabilities
    respond: "thistle1"
    clarify: "thistle1"
    message_generator: "thistle1"

    # Execution infrastructure
    python_generator: "sandy_brown"
    python_analyzer: "salmon1"
    python_executor: "misty_rose1"
    python_workflow: "light_steel_blue1"
    python_services: "light_steel_blue1"

    # Interface components
    cli: "deep_sky_blue1"
    pipeline: "deep_sky_blue1"

# ============================================================
# API PROVIDERS
# ============================================================

api:
  providers:
    cborg:
      api_key: ${CBORG_API_KEY}
      base_url: https://api.cborg.lbl.gov/v1
    stanford:
      api_key: ${STANFORD_API_KEY}
      base_url: https://aiapi-prod.stanford.edu/v1
    openai:
      api_key: ${OPENAI_API_KEY}
      base_url: https://api.openai.com/v1
    anthropic:
      api_key: ${ANTHROPIC_API_KEY}
      base_url: https://api.anthropic.com
    google:
      api_key: ${GOOGLE_API_KEY}
      base_url: https://generativelanguage.googleapis.com/v1beta
    ollama:
      api_key: ollama
      base_url: http://localhost:11434
      host: localhost
      port: 11434
    argo:
      api_key: ${ARGO_API_KEY}
      base_url: https://argo-bridge.cels.anl.gov

# ============================================================
# ENVIRONMENT VARIABLES
# ============================================================
# Set these in your .env file:
#
#   CBORG_API_KEY=your-cborg-key
#   STANFORD_API_KEY=...
#   ARGO_API_KEY=your-argo-key
#   OPENAI_API_KEY=sk-...
#   ANTHROPIC_API_KEY=sk-ant-...
#   GOOGLE_API_KEY=...
#   PROJECT_ROOT={{ project_root | default("/path/to/project") }}
#   LOCAL_PYTHON_VENV=/path/to/venv/bin/python
#   TZ=America/Los_Angeles
