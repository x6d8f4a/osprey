"""ARGO Provider Adapter Implementation.

ARGO is ANL's (Argonne National Laboratory) OpenAI-compatible proxy service.
This provider uses LiteLLM as the backend while preserving ARGO-specific
functionality like dynamic model list refresh.
"""

import logging
import os
from typing import Any

import httpx

from .base import BaseProvider
from .litellm_adapter import check_litellm_health, execute_litellm_completion

logger = logging.getLogger(__name__)


class ArgoProviderAdapter(BaseProvider):
    """ARGO (ANL) provider implementation using LiteLLM."""

    # Metadata (single source of truth)
    name = "argo"
    description = "ANL Argo proxy (supports multiple models)"
    requires_api_key = True
    requires_base_url = True
    requires_model_id = True
    supports_proxy = True
    default_base_url = "https://argo-bridge.cels.anl.gov"
    default_model_id = "claudesonnet45"  # Claude 4.5 Sonnet via ARGO for general use
    health_check_model_id = "gpt5mini"  # Fast and cost-effective for health checks
    available_models = [
        "claudehaiku45",
        "claudeopus41",
        "claudesonnet45",
        "claudesonnet37",
        "gemini25flash",
        "gemini25pro",
        "gpt5",
        "gpt5mini",
    ]
    _models_cache: list[str] | None = None

    # API key acquisition information
    api_key_url = None
    api_key_instructions = [
        "Argo uses the user login name which is obtained automatically from the $USER environment variable"
    ]
    api_key_note = None

    # LiteLLM integration - ARGO is an OpenAI-compatible proxy
    is_openai_compatible = True

    @classmethod
    def get_available_models(
        cls,
        api_key: str | None = None,
        base_url: str | None = None,
        force_refresh: bool = False,
    ) -> list[str]:
        """Dynamically fetch available models from the Argo /models endpoint.

        Falls back to static defaults if the request fails or credentials are missing.
        """
        if cls._models_cache is not None and not force_refresh:
            return cls._models_cache

        api_key = api_key or os.environ.get("ARGO_API_KEY")
        base_url = base_url or os.environ.get("ARGO_BASE_URL") or cls.default_base_url

        if not api_key or not base_url:
            cls._models_cache = cls.available_models
            return cls.available_models

        try:
            url = base_url.rstrip("/") + "/models"
            headers = {"Authorization": f"Bearer {api_key}"}
            with httpx.Client(timeout=5.0) as client:
                resp = client.get(url, headers=headers)
                resp.raise_for_status()
                data = resp.json()

            models: list[str] = []
            if isinstance(data, dict):
                raw_models = data.get("data", [])
                if isinstance(raw_models, list):
                    for item in raw_models:
                        if isinstance(item, dict):
                            model_id = item.get("id") or item.get("model") or item.get("name")
                            if model_id:
                                models.append(model_id)

            if models:
                cls.available_models = models
                cls._models_cache = models
                return models

            logger.debug("ARGO: /models returned no entries; using static defaults")
        except Exception as exc:
            logger.debug(f"ARGO: failed to refresh models from API ({exc}); using static defaults")

        cls._models_cache = cls.available_models
        return cls.available_models

    def execute_completion(
        self,
        message: str,
        model_id: str,
        api_key: str | None,
        base_url: str | None,
        max_tokens: int = 1024,
        temperature: float = 0.0,
        thinking: dict | None = None,
        system_prompt: str | None = None,
        output_format: Any | None = None,
        **kwargs,
    ) -> str | Any:
        """Execute ARGO chat completion via LiteLLM."""
        # Ensure models list is populated for any UI callers that rely on metadata
        try:
            self.get_available_models(api_key=api_key, base_url=base_url)
        except Exception:
            pass  # Don't block executions on model refresh errors

        return execute_litellm_completion(
            provider=self.name,
            message=message,
            model_id=model_id,
            api_key=api_key,
            base_url=base_url,
            max_tokens=max_tokens,
            temperature=temperature,
            output_format=output_format,
            **kwargs,
        )

    def check_health(
        self,
        api_key: str | None,
        base_url: str | None,
        timeout: float = 5.0,
        model_id: str | None = None,
    ) -> tuple[bool, str]:
        """Check ARGO API health via LiteLLM."""
        return check_litellm_health(
            provider=self.name,
            api_key=api_key,
            base_url=base_url,
            timeout=timeout,
            model_id=model_id or self.health_check_model_id,
        )
